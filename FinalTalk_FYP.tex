\documentclass{beamer}

%\usepackage[utf8]{inputenc}
\usepackage[english]{babel} 
\usetheme{Madrid}
\usepackage{amsmath,amssymb,amsthm,mathtools,cool}
\usepackage{enumitem}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\setitemize{label=\usebeamerfont*{itemize item}%
	\usebeamercolor[fg]{itemize item}
	\usebeamertemplate{itemize item}}
\setbeamertemplate{bibliography item}{\insertbiblabel}

\input{preamble}
%Information to be included in the title page:
\title[Introductory Talk] %optional
{Bias-Variance Tradeoff,
	Overfitting and the Double
	Descent Curve}





\begin{document}
	
\frame{\titlepage}

\begin{frame}
\frametitle{Main Papers}
\begin{itemize}
	\item \textit{Reconciling modern machine-learning practice and the classical bias-variance trade-off}, Belkin et al
	\item \textit{To undersand deep learning we need to understand kernel learning}, Belkin et al
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Introduction}
	
	\begin{block}{General Aim}
		Given training sample \[\{(x_1, y_1), ..., (x_n, y_n) \} \in \mathbb{R}^d \times \mathbb{R}\] where $(x_i,y_i)$ are i.i.d. variables drawn from probability distribution $P$,
		learn a predictor 
		$h_n : \mathbb{R}^d \to \mathbb{R}$ that predicts $y$ "well" given unseen $x$.
	\end{block}
	
	\begin{block}{Loss function $l$}
		Minimize loss: \\
		squared-loss function $l(y,\hat{y}) = (\hat{y} - y)^2$\\
		0-1 loss/ classification loss: $l(\hat{y},y) = 1_{\hat{y} \neq y}$
	\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Generalization}
\begin{itemize}[itemsep = 12pt]
	\item Find $h_n$ that performs well on unseen data.
	\item Minimize true risk: $h^*(x) = \argmin_h \EE[\ell (h(x), y)]$ where  $(x, y)$ drawn independently from $P$.
	\item Empirical Risk Minimization (ERM) goal: Minimize training risk. $L_{emp}(h_n) = \frac{1}{n} \sum_{i=1}^{n} l(h_n(x_i), y_i).$
	\[ \hat{h_n} = \arg\min_{h_n \in \HH} L_{emp}(h_n). \]
	\item $\mathcal{H}$ is a function class that contains functions approximating $h^*$.
	\item Classically, we do not choose function $h_n$ that reduces the empirical loss to near zero values, typically due to bounds on the generalization gap.
	
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Bias Variance Tradeoff}
\begin{itemize}[itemsep = 12pt]
	\item Finding a balance between underfitting and overfitting.
	\item "Bias-Variance Tradeoff"
	\item 0 training error does not tend to generalize well.
\end{itemize}
\begin{block}{Generalization Error}
	Difference Between empirical and expected classifier loss:
	$\EE_{x,y}[l(\hat{h_n}(x), y)] \leq L_{emp}(\hat{h_n}) + O(\sqrt{c/n})$\\
	where $c$ is some  measure of the complexity of $\HH$, for example the fat-shattering dimension, VC-dimension, etc.
\end{block}
\begin{itemize}[itemsep = 12pt]
	\item To control $L_{emp}$ and $c$, control $\HH$ implicitly or explicitly.
	\item Examples: Changing NN architectures, regularization, early stopping.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modern practice}
\begin{itemize}[itemsep = 12pt]
	\item Modern ML methods such as large neural networks and other non-linear predictors have very low to no training risk
	\item NN architectures chosen such that interpolation can be achieved.
	\item Works even when training data have high levels of noise.
\end{itemize}
\end{frame}

\begin{frame}
\includegraphics[width=11cm]{Good_Interpolation_Zhang.png}
\\ \textit{Understanding Deep Learning Requires Rethinking Generalization, Zhang et al} 
\end{frame}

\begin{frame}
\frametitle{Question}
\begin{itemize}[itemsep = 12pt]
	\item Unanswered as to why these overparameterized data do not seem to cause
	high test loss due to overfitting.
	\item Papers discussed further show empirically that this
	property is not exclusive to deep learning, but also seems to appear in learning for kernel
	machines as well.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Short introduction to Kernels and RKHS}
\begin{itemize}[itemsep = 12pt]
	\item Where $\KK$ may refer to either $\RR$ or $\CC$.
	\item For kernel $k: X \times X \rightarrow \KK $, there exists $\KK$-Hilbert space $\HH$ and map $\psi:X \rightarrow \HH$ such that for all $x_1, x_2 \in X$,
	\[ k(x_1, x_2) = \inner{\psi(x_2), \psi(x_1)}. \] 
	\item $\psi$ is called a feature map of $k$.
	\item (Real) Gaussian RBF kernel with width $\gamma$:
	\[ k_{\gamma}(x, x') := e^{\frac{-\norm{x - x'}_2^2}{\gamma^2}}. \]
	where $x, x' \in \RR^d$.
\end{itemize}
\end{frame}
\begin{frame}
\begin{block}{Positive Definite Functions}
	For a non-empty set $X$, a function $k: X \times X \rightarrow \RR$ is said to be a positive definite if, for all $x_1, ..., x_n \in X$, for all $a_1, ..., a_n \in \RR$, we have:
	\[ \sum_{j=1}^{n} \sum_{i=1}^{n} a_j a_i k(x_j, x_i) \geq 0. \]
\end{block}
\begin{block}{Positive Definite Symmetric Functions are Kernels}
A real function $k:X \times X \rightarrow \RR$ is a kernel if and only if $k$ is a positive definite symmetric function.
\end{block}
\end{frame}

\begin{frame}
\frametitle{Short introduction to Kernels and RKHS}
Reproducing kernel Hilbert spaces have many applications in the fields such as Statistical Learning and complex analysis. An RKHS is a
K-Hilbert function space where point evaluation is continuous linear funcitonal.

\begin{block}{Reproducing Kernel Hilbert Spaces (\textbf{RKHS}) Definition}
Let $\mathcal{H}$ be a $\KK$-Hilbert space of functions over a non-empty set $X$. $\HH$ is called an RKHS over $X$ if the Dirac function $\delta_x: \HH \rightarrow \KK$ defined as:
\[ \delta_x(f) := f(x), ~ x \in X, ~ f \in \HH \] is continuous.
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Reproducing Kernels}
For a non-empty set $X$ and a function $k : X \times X \rightarrow \KK$ $k$ is called a reproducing kernel of $\HH$ (a Hilbert function space) if $k(\cdot, x) \in \HH$ for all $x \in X$ and the following property hold for all $x \in X$ and $f \in \HH$:
\begin{equation*}
f(x) = \inner{f, k(\cdot, x)}
\end{equation*}
This condition is also known as the reproducing property. 
\end{block}
\begin{itemize}
	\item It can be shown that $k$ is a kernel.
	\item $\HH$ is then an RKHS.
\end{itemize}
\end{frame}

\begin{frame} \frametitle{Representer Theorem}
Representer Theorem ensures that the argmin of an empirical risk expression involving a
function over an RKHS can be expressed as a linear combination of kernels applied on the
training data points as proven in Scholkopf et al.
\end{frame}

\begin{frame}
\begin{block}{Representer Theorem}
	Training data $(x_1,y_1), ... (x_n,y_n) \in X \times \RR$, and RKHS $\mathcal{H}$ a $\RR$-Hilbert function space over $X$ with reproducing kernel $k:X \times X \rightarrow \RR$.\\ Let $g$ be a strictly increasing function $g:[0, \infty]\rightarrow \RR$, and $l:(X \times \RR^2)^n \rightarrow \RR \cup \{\infty\}$ be an arbitrary loss function. \\
	We want to minimize the following empirical risk term:
	$L_{emp}(f, (x_1, y_1), ..., (x_n, y_n)) ~ := ~ l((x_1, y_1, f(x_1)), ..., (x_n, y_n, f(x_n))) + g(\norm{f})$\\
	For $\hat{f} = \arg \min_{f \in \mathcal{H}} L_{emp}(f, (x_1, y_1), ..., (x_n, y_n))$, $\hat{f}$ can be represented in the form:
	\[ \hat{f}(\cdot) = \sum_{i=1}^{n} a_i k(\cdot, x_i) \]
	with $a_i \in \RR$ for $1 \leq i \leq n$.
\end{block}
\end{frame}

\begin{frame} \frametitle{Proof Sketch}
\begin{itemize} \setlength\itemsep{1em}
	\item Let $\Phi(x)(\cdot) = k(\cdot , x)$.
	\item $f = \sum_{i=1}^{n} a_i \Phi(x_i) + \gamma$, where $\inner{\Phi(x_i), \gamma} = 0$
	\item $f(x_j) = \sum_{i=1}^{n} a_i \inner{\Phi(x_i),  \Phi(x_j)}$
	\item So $f(x_j)$ is unaffected by $\gamma$.
	\item $\norm{f}^2 = \norm{\sum_{i=1}^{n} a_i \Phi(x_i) + \gamma}^2 \geq \norm{\sum_{i=1}^{n} a_i \Phi(x_i)}^2$
	\item $g(\norm{f}) \geq g(\norm{\sum_{i=1}^{n} a_i \Phi(x_i)})$
	\item So $\gamma = 0$ and $\hat{f} = \sum_{i=1}^{n} a_i k(\cdot, x_i)$
\end{itemize}
\end{frame}

\begin{frame}{Overfitted and Interpolated Kernel Classifiers}
\begin{itemize}
	\item Performed in Belkin et al. (2018b) that show the strong
	generalization performance also appears in kernel classifiers.
	\item The aim is to have interpolated solutions which fits the data perfectly, thus having no
	regularization.
	\item Though no finite number of functions in the RKHS are able to
	fit the training data, minimum norm RKHS solutions can be obtained using Representer
	Theorem.
\end{itemize}
\end{frame}

\begin{frame}{Training}
\begin{itemize} \setlength\itemsep{1em}
	\item Training datapoints $(x_1. y_1), ..., (x_n, y_n) \in \RR^d \times \RR$.
	\item Let $\HH$ denote the RKHS corresponding to the kernel $k$.
	\item Let $f^*$ denote the minimum norm interpolant given the datapoints.
	\item $f^*(\cdot) = \sum_{i=1}^{n} a^*_i k(x_i, \cdot).$
	\item Due to its interpolating properties, we know that for $a^* := (a^*_1, ..., a^*_n)^\Trans$.
	\item $\norm{f^*}_\HH = \sum_{i,j=1}^{n} a^*_i a^*_j k(x_i, x_j) = a^{*\Trans} K a^*$
	\item $a^* = \arg \min_{a} \sum_{i} l((\sum_{j}a_i k(x_j,x_i)) , y_i)$ for some convex loss $l$.
\end{itemize}
\end{frame}

\begin{frame}{Results}
\begin{itemize} \setlength\itemsep{1em}
	\item Models were trained till mean square loss on training dataset approaches zero.
	\item The interpolated solution performed close to optimal.
	\item The benefits of early stopping regularization were little to none in terms of test
	regression or classification error.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Interpolated Kernels Performance}
\includegraphics[height=6cm]{Interpolated_Kernel_1.png}
\end{frame}

\begin{frame}{Existing Bounds}
\begin{itemize} \setlength\itemsep{1em}
	\item How the norm of the classifiers change w.r.t. data size.
	\item  classifiers are called overfitted if the classification
	loss for the training set is 0 or near 0, and classifiers are called interpolated when the
	mean square error is 0 or near 0.
\end{itemize}
\begin{block}{t-overfitting}
For a function $h \in \HH$, and $t \in \RR^+$, it \textbf{t-overfits} the data if it is overfitted with 0 classification loss, and $0 < t< y_ih(x_i)$ for $1\leq i \leq n$.
\end{block}
Introduced to prevent arbitrary scaling of solution.
\end{frame}

\begin{frame}
\begin{block}{Bounds of t-overfitters}
	Let $(x_1 ,y_1),..., (x_n,x_n) \in \Omega \times \{-1, 1\}$ be data sampled from probability distribution $P$, and $y$ is not a deterministic function of $x$. With high probability, for any $h$ that t-overfits the data, there exists some constants  $A, B > 0$ depending on $t$, that satisfies
	\[\norm{h}_\HH > Ae^{Bn^{1/d}}. \]
\end{block}
\begin{block}{Classical Bounds}
	Classical bounds for kernel methods are in the form:
	\[ \abs{\frac{1}{n} \sum_{i=1}^{n}l(f(x_i),y_i) - L(f)} \leq C_1 \frac{\norm{f}^a_\mathcal{H}}{n^b}, \hspace{1em} C_1, a,b \geq 0 \]
	The right side on this will tend to infinity for bigger $\norm{f}_\mathcal{H}$.
\end{block}
\end{frame}

\begin{frame}
\frametitle{Classical Descent Curve}
\includegraphics[height=6cm]{UCurve.png}
\\Classical curve from bias variance tradeoff.
\end{frame}

\begin{frame}
\frametitle{"Double Descent"}
\begin{itemize}[itemsep = 12pt]
	\item "Double Descent" curve proposed and empirically observed to some extent.
	\item Curve the extends beyond the point of interpolation
	\item Risk decreases beyond this point, typically surpassing performance of classical stopping point.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Double Descent Curve}
\includegraphics[height=5cm]{Double-Descent-Curve.png}
\end{frame}
\begin{frame}{Random Fourier Features}
We want to find a randomized feature map $z:\RR^d \rightarrow \RR^D$ such that:
$k(x,y) = \inner{\phi(x), \phi(y)} \approx \inner{z(x), z(y)}.$
\begin{block}{}
(Bochner Rudin (1990)).For a continuous (properly scaled) kernel $k(x - y)$  it is a positive definite kernel if and only if $k(\cdot)$ is the fourier transform of a non-negative measure.
\end{block}
\begin{block}{}
\[ k(x - y) = \int_{\RR^d} p(w) \text{exp}(iw^{\Trans}(x - y)) ~ \text{d}w = \EE_w[e^{iw^{\Trans}x}(e^{iw^{\Trans}y})^*]. \]
\end{block}
\end{frame}
\begin{frame}
\frametitle{Random Fourier Features}
Let the function class  $\mathcal{H}_\infty$ be the Reproducing Kernel Hilbert Space (RKHS) corresponding to the Gaussian kernel.\\
We consider the following non-linear parametric model:
\begin{block}{Random Fourier Features (RFF) \\ \small{\textit{Random Features for Large-Scale Kernel Machines} (Rahimi et al)}}
	Let the function class  $\mathcal{H}_N $ consist of functions $h_n : \mathbb{R}^d \to \mathbb{C}$ of the form:
	\[ h(\cdot) = \sum_{k=1}^{N} a_k\phi(\cdot, v_k) \] 
	where $\phi(\cdot , v_k) := e^{i
	\langle v_k , \cdot \rangle }$
	vectors $v_1, ... , v_N$ are sampled independently from the standard normal distribution in $\mathbb{R}^d$.
\end{block}
$H_N$ has $N$ parameters in $\mathbb{C}$, \{ $a_1, ..., a_N$ \}.\\
As $N \to \infty$, $H_N$ becomes a closer approximation to $\mathcal{H}_\infty$
\end{frame}

\begin{frame}
\frametitle{Empirical Evidence - Learning Procedure}
\begin{itemize}[itemsep = 12pt]
	\item Given training sample $(x_1, y_1), ..., (x_n, y_n) \in \mathbb{R}^d \times \mathbb{R}$.
	\item Minimize empirical risk: $\frac{1}{n}\sum_{j=1}^{n}(h(x_j)-y_j)^2$ for $h \in \mathcal{H}_N$.
	\item When minimizer not unique ($N > n$), choose the minimizer with the coefficients $(a_1, ..., a_N)$ that have the smallest $\ell_2$ norm.
	\item Let this predictor be: $h_{n,N} \in \mathcal{H}_N$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Empirical Evidence - Results}
\centering
\includegraphics[height=7cm]{RFF-results.png}
\end{frame}

\begin{frame}
\frametitle{Explanations on Double Descent}
Why should the test risk decrease even when empirical risk stays the same?\\
\begin{block}{Classical Bounds}
$\EE_{x,y}[l(\hat{h_n}(x), y)] \leq L_{emp}(\hat{h_n}) + O(\sqrt{c/n})$\
\end{block}
\begin{itemize}[itemsep = 12pt]
\item The inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered
\item Capacity of function class needs not suit the appropriate inductive bias for the problem.
\item By having a larger function class, may find a function that matches the inductive bias better. Eg., smoother function, smaller norm, larger margin.
\item In this case, a smaller norm.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Min Norm  RKHS solution $h_{n, \infty}$}
\begin{itemize}[itemsep = 12pt]
	\item $f^* = \argmin_{f\in\RKHSGauss, f(x_i) = y_i} \norm{f}_{\RKHSGauss}$ 
	\item $f^*(\cdot) =\sum_{i=1}^{n} \alpha_i k(\cdot, x_i) $ (Representer theorem)
	\item Since $f^*$ interpolates, $(\alpha_1,..., \alpha_n)^{\Trans} = K^{-1} (y_1,..., y_n)^{\Trans}$
	\item $\norm{f}_{\RKHSGauss}^2 = \sum_{i,j} \alpha_i \alpha_j K(x_i,x_j) $
	\begin{block}{Approximation theorem}
		Fix $h^* \in \mathcal{H}_\infty $  .
		Let $(x_1,y_1), ..., (x_n,y_n)$ be i.i.d. random variables where $x_i$ drawn randomly from a compact cube $\Omega \subset \mathbb{R}^d $,
		$y_i = h^*(x_i) \: \forall i$. There exists $A, B > 0$ such that for any interpolating $h \in \mathcal{H}_\infty $ with high probability
		\[ \sup_{x \in \Omega} \vert h(x) - h^*(x)\vert < A e^{-B(n/log \, n)^{1/d}} (\| h^* \|_{\mathcal{H}_\infty} + \| h \|_{\mathcal{H}_\infty}) \]
	\end{block}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Empirical Evidence - Other Examples}
\centering
\includegraphics[height=7cm]{Double_Descent_Examples.png}
\end{frame}

%\begin{frame}
%\frametitle{Appendix on Approximation Theorem}
%\begin{block}{Approximation theorem}
%	Fix $h^* \in \mathcal{H}_\infty $  .
%	Let $(x_1,y_1), ..., (x_n,y_n)$ be i.i.d. random variables where $x_i$ drawn randomly from a compact cube $\Omega \subset \mathbb{R}^d $,
%	$y_i = h^*(x_i) \: \forall i$. There exists $A, B > 0$ such that for any interpolating $h \in \mathcal{H}_\infty $ with high probability
%	\[ \sup_{x \in \Omega} \vert h(x) - h^*(x)\vert < A e^{-B(n/log \, n)^{1/d}} (\| h^* \|_{\mathcal{H}_\infty} + \| h \|_{\mathcal{H}_\infty}) \]
%\end{block}
%\end{frame}

%\begin{frame}
%\frametitle{Main References}
%\begin{thebibliography}{1}
%	\bibitem{Kernel Learning} 
%	Mikhail Belkin, Siyuan Ma, and Soumik Mandal. 
%	\textit{To understand deep learning we need to understand kernel learning.} 
%	In Proceedings of the 25th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 541-549, 2018.
%\end{thebibliography}
%\end{frame}
\end{document} 
